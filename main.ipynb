{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--T55fwGxT9m"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PRuWNslpy6M"
      },
      "outputs": [],
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyzbXbHPp2-9"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSDQaBhRsAGE"
      },
      "outputs": [],
      "source": [
        "# Standardize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMPQmJP2qJn8"
      },
      "outputs": [],
      "source": [
        "# Define the parameter grid for GridSearchCV to find the best hyperparameters\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(10,), (30,), (50,), (10,10), (30,20)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'max_iter': [500, 1000]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LAuSwnxrbTy"
      },
      "outputs": [],
      "source": [
        "# Initialize MLPClassifier and perform GridSearchCV to find the best model\n",
        "mlp = MLPClassifier(random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(mlp, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Print the best parameters and best score found by GridSearchCV\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best score: \", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7_bMI5EsQJK"
      },
      "outputs": [],
      "source": [
        "# Set the best parameters to the MLPClassifier and train the model\n",
        "mlp.set_params(**grid_search.best_params_)\n",
        "mlp.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz6V0cP_upSN"
      },
      "outputs": [],
      "source": [
        "# Function to calculate evaluation metrics (accuracy, precision, recall, F-score, specificity)\n",
        "def evaluate_metrics(tn, fp, fn, tp):\n",
        "  accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "  precision = tp / (tp + fp)\n",
        "  recall = tp / (tp + fn)\n",
        "  f_score = 2 * (precision * recall) / (precision + recall)\n",
        "  specificity = tn / (tn + fp)\n",
        "  return accuracy, precision, recall, f_score, specificity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hysrvVQdxDI1"
      },
      "outputs": [],
      "source": [
        "# Predict on the test set and compute the confusion matrix\n",
        "y_pred = mlp.predict(X_test_scaled)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Initialize accumulators for metrics\n",
        "accuracy_acm = 0\n",
        "precision_acm = 0\n",
        "recall_acm = 0\n",
        "f_score_acm = 0\n",
        "specificity_acm = 0\n",
        "\n",
        "# Calculate and print metrics for each class\n",
        "for i in range(len(iris.target_names)):\n",
        "    tn = cm.sum() - (cm[i, :].sum() + cm[:, i].sum() - cm[i, i])\n",
        "    fp = cm[:, i].sum() - cm[i, i]\n",
        "    fn = cm[i, :].sum() - cm[i, i]\n",
        "    tp = cm[i, i]\n",
        "\n",
        "    accuracy, precision, recall, f_score, specificity = evaluate_metrics(tn, fp, fn, tp)\n",
        "\n",
        "    accuracy_acm += accuracy\n",
        "    precision_acm += precision\n",
        "    recall_acm += recall\n",
        "    f_score_acm += f_score\n",
        "    specificity_acm += specificity\n",
        "\n",
        "    print(f\"Metrics for class {iris.target_names[i]}:\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F-score: {f_score}\")\n",
        "    print(f\"Specificity: {specificity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ixIoBd_298M"
      },
      "outputs": [],
      "source": [
        "# Calculate and print overall metrics (averaged across all classes)\n",
        "accuracy_overall = accuracy_acm / len(iris.target_names)\n",
        "precision_overall = precision_acm / len(iris.target_names)\n",
        "recall_overall = recall_acm / len(iris.target_names)\n",
        "f_score_overall = f_score_acm / len(iris.target_names)\n",
        "specificity_overall = specificity_acm / len(iris.target_names)\n",
        "\n",
        "print(\"Metrics for the overall model:\")\n",
        "print(f\"Average Accuracy: {accuracy_overall}\")\n",
        "print(f\"Average Precision: {precision_overall}\")\n",
        "print(f\"Average Recall: {recall_overall}\")\n",
        "print(f\"Average F-score: {f_score_overall}\")\n",
        "print(f\"Average Specificity: {specificity_overall}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiUJBz8P4U-p"
      },
      "outputs": [],
      "source": [
        "# Binarize the labels for ROC curve calculation\n",
        "y_test_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "y_score = mlp.predict_proba(X_test_scaled)\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(3):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot the ROC curves for each class\n",
        "plt.figure()\n",
        "colors = ['blue', 'red', 'green']\n",
        "for i, color in zip(range(3), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, label=f'ROC curve of class {i} (area = {roc_auc[i]:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Multi-class ROC curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
